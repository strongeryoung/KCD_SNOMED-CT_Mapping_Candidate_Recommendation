{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GoIAH8U9Ap3"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Colab í™˜ê²½ìš© í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ì…€ 1ê°œ ì‹¤í–‰ìš©)\n",
        "\n",
        "ê°œì„ ëœ SapBERT ê¸°ë°˜ KCD â†’ SNOMED ë§¤í•‘ ì‹œìŠ¤í…œ\n",
        "- Google Drive ë§ˆìš´íŠ¸ + ê²½ë¡œ ì„¤ì •\n",
        "- SapBERT ì„ë² ë”© + (ì˜µì…˜) FAISS ê³ ì† ê²€ìƒ‰\n",
        "- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Lexical + Semantic)\n",
        "- Re-ranking ì „ëµ\n",
        "- Accuracy@1,5,10 / Top-K / MRR / NDCG@5,10 / MAP@10 / Avg Rank\n",
        "- ì‹œê°í™” + ê²°ê³¼/ì§€í‘œ ì €ì¥\n",
        "\"\"\"\n",
        "\n",
        "# ================================\n",
        "# 0. ê¸°ë³¸ ì„¤ì • ë° ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "# ================================\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda if torch.cuda.is_available() else \"N/A\")\n",
        "\n",
        "# Colab ì „ìš©: Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ================================\n",
        "# 1. ê²½ë¡œ ì„¤ì • (í•„ìš”ì‹œ ìˆ˜ì •)\n",
        "# ================================\n",
        "BASE_DIR   = \"/content/drive/MyDrive/misoProject/base\"\n",
        "SOURCE_DIR = os.path.join(BASE_DIR, \"source_data\")\n",
        "TARGET_DIR = os.path.join(BASE_DIR, \"target_data\")\n",
        "TEST_DIR   = os.path.join(BASE_DIR, \"test\")\n",
        "CACHE_DIR  = os.path.join(BASE_DIR, \"cache\")\n",
        "\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "print(\"ğŸ“‚ BASE_DIR:\", BASE_DIR)\n",
        "print(\"ğŸ“‚ ë‚´ìš©:\", os.listdir(\".\"))\n",
        "print(\"ğŸ“‚ source_data:\", os.listdir(SOURCE_DIR))\n",
        "print(\"ğŸ“‚ target_data:\", os.listdir(TARGET_DIR))\n",
        "print(\"ğŸ“‚ test:\", os.listdir(TEST_DIR))\n",
        "\n",
        "# ================================\n",
        "# 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
        "# (preprocessing_ver2_module.ipynb ë‚´ìš©ì„ ì—¬ê¸°ë¡œ ì˜®ê²¼ë‹¤ëŠ” ê°€ì •)\n",
        "# í•„ìš”ì— ë”°ë¼ ì‹¤ì œ êµ¬í˜„ìœ¼ë¡œ êµì²´\n",
        "# ================================\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    # ì˜ˆì‹œ: ì†Œë¬¸ìí™” + NFKC ì •ê·œí™” + ê¸°í˜¸ ì œê±° ë“±\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = text.lower()\n",
        "    # ìˆ«ì/ì˜ë¬¸/ê³µë°±ë§Œ ë‚¨ê¸°ê¸° (í•„ìš”ì‹œ ìˆ˜ì •)\n",
        "    allowed = string.ascii_lowercase + string.digits + \" \"\n",
        "    text = \"\".join(ch if ch in allowed else \" \" for ch in text)\n",
        "    # ë‹¤ì¤‘ ê³µë°± ì •ë¦¬\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "# ë¶ˆìš©ì–´ ì œê±° (ì˜ë¬¸ ê°„ë‹¨ ì˜ˆì‹œ, ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ëŠ” í™•ì¥ ê°€ëŠ¥)\n",
        "STOPWORDS = set([\"of\", \"the\", \"and\", \"a\", \"an\", \"in\", \"on\", \"with\", \"for\", \"to\"])\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# ë‹¨ìˆœ ë³µìˆ˜í˜• ì •ê·œí™” (ì˜ˆì‹œ)\n",
        "def normalize_plural(text: str) -> str:\n",
        "    tokens = text.split()\n",
        "    norm_tokens = []\n",
        "    for t in tokens:\n",
        "        if t.endswith(\"s\") and len(t) > 3:\n",
        "            norm_tokens.append(t[:-1])\n",
        "        else:\n",
        "            norm_tokens.append(t)\n",
        "    return \" \".join(norm_tokens)\n",
        "\n",
        "def apply_morph(text: str) -> str:\n",
        "    \"\"\"ê°„ë‹¨ í˜•íƒœì†Œ/í† í° ë¶„ì„ê¸° (í˜„ì¬ëŠ” í† í°í™” ìˆ˜ì¤€, í•„ìš”ì‹œ Mecab ë“± ì—°ë™)\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    x = normalize_text(text)\n",
        "    tokens = x.split()\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "print(\"âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
        "print(\"ì˜ˆì‹œ normalize_text('Test TEXT!'):\", normalize_text(\"Test TEXT!\"))\n",
        "\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€\n",
        "USE_MORPH_FOR_KCD    = False\n",
        "USE_MORPH_FOR_SNOMED = False\n",
        "\n",
        "print(f\"USE_MORPH_FOR_KCD    = {USE_MORPH_FOR_KCD}\")\n",
        "print(f\"USE_MORPH_FOR_SNOMED = {USE_MORPH_FOR_SNOMED}\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 3. SNOMED TSV ì•ˆì „ ë¡œë”\n",
        "# ================================\n",
        "def load_snomed_tsv(path: str, usecols=None) -> pd.DataFrame:\n",
        "    encodings = [\n",
        "        \"utf-8\", \"utf-8-sig\",\n",
        "        \"utf-16\", \"utf-16le\", \"utf-16be\",\n",
        "        \"latin1\", \"cp1252\", \"ISO-8859-1\",\n",
        "    ]\n",
        "    last_err = None\n",
        "    for enc in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                path,\n",
        "                sep=\"\\t\",\n",
        "                dtype=str,\n",
        "                low_memory=False,\n",
        "                encoding=enc,\n",
        "                usecols=usecols\n",
        "            )\n",
        "            print(f\"âœ… {os.path.basename(path)}: decoded with {enc}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raise RuntimeError(f\"â— Failed to decode {path}. Last error: {last_err}\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. SNOMED íŒŒì¼ ê²½ë¡œ ê°ì§€ ë° ë¡œë“œ\n",
        "# ================================\n",
        "concept_file = [f for f in os.listdir(TARGET_DIR) if \"Concept_Snapshot\" in f][0]\n",
        "desc_file    = [f for f in os.listdir(TARGET_DIR) if \"Description_Snapshot\" in f][0]\n",
        "rel_file     = [f for f in os.listdir(TARGET_DIR) if \"Relationship_Snapshot\" in f][0]\n",
        "\n",
        "concept_path = os.path.join(TARGET_DIR, concept_file)\n",
        "desc_path    = os.path.join(TARGET_DIR, desc_file)\n",
        "rel_path     = os.path.join(TARGET_DIR, rel_file)\n",
        "\n",
        "print(\"Concept  :\", concept_path)\n",
        "print(\"Desc     :\", desc_path)\n",
        "print(\"Relation :\", rel_path)\n",
        "\n",
        "concept_cols = [\"id\", \"active\"]\n",
        "df_concept = load_snomed_tsv(concept_path, usecols=concept_cols)\n",
        "print(\"Concept raw shape:\", df_concept.shape)\n",
        "\n",
        "desc_cols = [\"id\", \"active\", \"languageCode\", \"conceptId\", \"typeId\", \"term\"]\n",
        "df_desc = load_snomed_tsv(desc_path, usecols=desc_cols)\n",
        "print(\"Desc raw shape:\", df_desc.shape)\n",
        "\n",
        "rel_cols = [\"active\", \"sourceId\", \"destinationId\", \"typeId\"]\n",
        "df_rel = load_snomed_tsv(rel_path, usecols=rel_cols)\n",
        "print(\"Rel raw shape:\", df_rel.shape)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 5. Concept/Description/Relation ì „ì²˜ë¦¬\n",
        "# ================================\n",
        "df_concept = df_concept[df_concept[\"active\"] == \"1\"][[\"id\"]].copy()\n",
        "df_concept = df_concept.rename(columns={\"id\": \"conceptId\"})\n",
        "print(\"Concept active=1:\", len(df_concept))\n",
        "\n",
        "df_desc = df_desc[\n",
        "    (df_desc[\"active\"] == \"1\") &\n",
        "    (df_desc[\"languageCode\"] == \"en\")\n",
        "][[\"conceptId\", \"typeId\", \"term\"]].copy()\n",
        "print(\"Desc active=1 & en:\", len(df_desc))\n",
        "\n",
        "FSN_ID = \"900000000000003001\"\n",
        "SYN_ID = \"900000000000013009\"\n",
        "\n",
        "def extract_semantic_tag(term: str) -> str:\n",
        "    m = re.search(r\"\\(([^()]*)\\)\\s*$\", str(term))\n",
        "    return m.group(1).lower() if m else \"\"\n",
        "\n",
        "df_fsn = df_desc[df_desc[\"typeId\"] == FSN_ID][[\"conceptId\", \"term\"]].copy()\n",
        "df_fsn[\"semantic_tag\"] = df_fsn[\"term\"].apply(extract_semantic_tag)\n",
        "\n",
        "df_fsn_disorder = df_fsn[df_fsn[\"semantic_tag\"] == \"disorder\"].copy()\n",
        "disorder_ids = set(df_fsn_disorder[\"conceptId\"])\n",
        "print(\"disorder concept ê°œìˆ˜:\", len(disorder_ids))\n",
        "\n",
        "del df_fsn, df_fsn_disorder\n",
        "gc.collect()\n",
        "\n",
        "df_desc = df_desc[df_desc[\"conceptId\"].isin(disorder_ids)].copy()\n",
        "print(\"Desc (disorder only):\", len(df_desc))\n",
        "\n",
        "df_rel = df_rel[df_rel[\"active\"] == \"1\"][[\"sourceId\", \"destinationId\", \"typeId\"]].copy()\n",
        "print(\"Rel active=1:\", len(df_rel))\n",
        "\n",
        "IS_A = \"116680003\"\n",
        "df_rel_isa = df_rel[df_rel[\"typeId\"] == IS_A][[\"sourceId\", \"destinationId\"]].copy()\n",
        "print(\"is-a ê´€ê³„ í–‰ìˆ˜:\", len(df_rel_isa))\n",
        "\n",
        "del df_rel\n",
        "gc.collect()\n",
        "\n",
        "df_parent = (\n",
        "    df_rel_isa\n",
        "    .groupby(\"sourceId\")[\"destinationId\"]\n",
        "    .apply(list)\n",
        "    .reset_index()\n",
        "    .rename(columns={\"sourceId\": \"conceptId\", \"destinationId\": \"parent_concepts\"})\n",
        ")\n",
        "\n",
        "df_child = (\n",
        "    df_rel_isa\n",
        "    .groupby(\"destinationId\")[\"sourceId\"]\n",
        "    .apply(list)\n",
        "    .reset_index()\n",
        "    .rename(columns={\"destinationId\": \"conceptId\", \"sourceId\": \"child_concepts\"})\n",
        ")\n",
        "\n",
        "print(\"parent ë¦¬ìŠ¤íŠ¸ í–‰ìˆ˜:\", len(df_parent))\n",
        "print(\"child  ë¦¬ìŠ¤íŠ¸ í–‰ìˆ˜:\", len(df_child))\n",
        "\n",
        "del df_rel_isa\n",
        "gc.collect()\n",
        "\n",
        "def map_term_type(tid: str) -> str:\n",
        "    if tid == FSN_ID:\n",
        "        return \"FSN\"\n",
        "    elif tid == SYN_ID:\n",
        "        return \"SYN\"\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "df_desc[\"term_type\"] = df_desc[\"typeId\"].apply(map_term_type)\n",
        "\n",
        "def normalize_for_snomed(text: str) -> str:\n",
        "    x = normalize_text(text)\n",
        "    if USE_MORPH_FOR_SNOMED:\n",
        "        x = apply_morph(x)\n",
        "    x = remove_stopwords(x)\n",
        "    x = normalize_plural(x)\n",
        "    return x\n",
        "\n",
        "df_desc[\"normalized_term\"] = df_desc[\"term\"].apply(normalize_for_snomed)\n",
        "\n",
        "print(\"df_desc normalized_term ì˜ˆì‹œ:\")\n",
        "print(df_desc[[\"term\", \"term_type\", \"normalized_term\"]].head())\n",
        "\n",
        "df_snomed = df_desc.merge(\n",
        "    df_concept[[\"conceptId\"]],\n",
        "    on=\"conceptId\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "print(\"Concept + Desc ì¡°í•© í–‰ìˆ˜:\", len(df_snomed))\n",
        "\n",
        "del df_concept, df_desc\n",
        "gc.collect()\n",
        "\n",
        "df_snomed = df_snomed.merge(df_parent, on=\"conceptId\", how=\"left\")\n",
        "df_snomed = df_snomed.merge(df_child, on=\"conceptId\", how=\"left\")\n",
        "\n",
        "del df_parent, df_child\n",
        "gc.collect()\n",
        "\n",
        "dedup_cols = [\"conceptId\", \"term\", \"term_type\", \"normalized_term\"]\n",
        "snomed_index = (\n",
        "    df_snomed\n",
        "    .drop_duplicates(subset=dedup_cols)\n",
        "    .reset_index(drop=True)\n",
        "    [[\"conceptId\", \"term\", \"term_type\", \"normalized_term\",\n",
        "      \"parent_concepts\", \"child_concepts\"]]\n",
        ")\n",
        "\n",
        "del df_snomed\n",
        "gc.collect()\n",
        "\n",
        "print(\"âœ… ìµœì¢… snomed_index í–‰ìˆ˜:\", len(snomed_index))\n",
        "print(snomed_index.head(5))\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. TEST ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "# ================================\n",
        "test_file = os.path.join(TEST_DIR, \"test_source.xlsx\")\n",
        "df_test = pd.read_excel(test_file, dtype=str).reset_index(drop=True)\n",
        "print(\"âœ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ:\", df_test.shape)\n",
        "print(df_test.head())\n",
        "\n",
        "if \"source_code\" in df_test.columns:\n",
        "    df_test = df_test.rename(columns={\"source_code\": \"kcd_code\"})\n",
        "if \"source_en_name\" in df_test.columns:\n",
        "    df_test = df_test.rename(columns={\"source_en_name\": \"kcd_en\"})\n",
        "\n",
        "df_test[\"kcd_en\"] = df_test[\"kcd_en\"].fillna(\"\")\n",
        "\n",
        "def normalize_for_kcd(text: str) -> str:\n",
        "    x = normalize_text(text)\n",
        "    if USE_MORPH_FOR_KCD:\n",
        "        x = apply_morph(x)\n",
        "    x = remove_stopwords(x)\n",
        "    x = normalize_plural(x)\n",
        "    return x\n",
        "\n",
        "df_test[\"kcd_normalized\"] = df_test[\"kcd_en\"].apply(normalize_for_kcd)\n",
        "df_test[\"kcd_no_sw\"]      = df_test[\"kcd_normalized\"]\n",
        "df_test[\"kcd_no_plural\"]  = df_test[\"kcd_normalized\"].apply(normalize_plural)\n",
        "\n",
        "print(\"âœ” ì „ì²˜ë¦¬ ì˜ˆì‹œ:\")\n",
        "print(df_test[[\"kcd_en\",\"kcd_normalized\",\"kcd_no_sw\",\"kcd_no_plural\"]].head())\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 7. SapBERT ë¡œë“œ + SNOMED ì„ë² ë”©\n",
        "# ================================\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"ğŸ”¥ Using device:\", device)\n",
        "\n",
        "sapbert = SentenceTransformer(\n",
        "    \"cambridgeltl/SapBERT-UMLS-2020AB-all-lang-from-XLMR\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "def encode_in_batches(model, texts, batch_size=32, cache_path=None):\n",
        "    \"\"\"ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ  ì¸ì½”ë”©í•˜ê³  ìºì‹±\"\"\"\n",
        "    if cache_path and os.path.exists(cache_path):\n",
        "        print(f\"âœ… ìºì‹œì—ì„œ ë¡œë“œ: {cache_path}\")\n",
        "        return np.load(cache_path)\n",
        "\n",
        "    embeddings = []\n",
        "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        batch_emb = model.encode(\n",
        "            batch,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True,\n",
        "            show_progress_bar=False\n",
        "        )\n",
        "        embeddings.append(batch_emb)\n",
        "\n",
        "        if (i // batch_size) % 100 == 0:\n",
        "            print(f\"  ì§„í–‰: {i // batch_size + 1}/{total_batches} ë°°ì¹˜\")\n",
        "\n",
        "    embeddings = np.vstack(embeddings).astype(\"float32\")\n",
        "\n",
        "    if cache_path:\n",
        "        np.save(cache_path, embeddings)\n",
        "        print(f\"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_path}\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "SAPBERT_CACHE = os.path.join(CACHE_DIR, \"snomed_sapbert_embs_v2.npy\")\n",
        "snomed_terms = snomed_index[\"normalized_term\"].fillna(\"\").tolist()\n",
        "\n",
        "print(\"ğŸ”„ SNOMED ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
        "snomed_embs = encode_in_batches(\n",
        "    sapbert,\n",
        "    snomed_terms,\n",
        "    batch_size=32,\n",
        "    cache_path=SAPBERT_CACHE\n",
        ")\n",
        "print(f\"âœ… SNOMED ì„ë² ë”© ì™„ë£Œ: {snomed_embs.shape}\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 8. (ì˜µì…˜) FAISS ì„¤ì •\n",
        "# ================================\n",
        "try:\n",
        "    import faiss\n",
        "    FAISS_AVAILABLE = True\n",
        "    print(\"âœ… FAISS ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except ImportError:\n",
        "    FAISS_AVAILABLE = False\n",
        "    print(\"âš ï¸ FAISS ë¯¸ì„¤ì¹˜ - sklearn ì‚¬ìš©\")\n",
        "\n",
        "def build_faiss_index(embeddings, use_gpu=True):\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)\n",
        "    if use_gpu and faiss.get_num_gpus() > 0:\n",
        "        res = faiss.StandardGpuResources()\n",
        "        index = faiss.index_cpu_to_gpu(res, 0, index)\n",
        "        print(\"ğŸ”¥ GPU FAISS ì‚¬ìš©\")\n",
        "    else:\n",
        "        print(\"ğŸ’» CPU FAISS ì‚¬ìš©\")\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def faiss_topk_search(index, query_embs, k=5):\n",
        "    scores, indices = index.search(query_embs, k)\n",
        "    return indices, scores\n",
        "\n",
        "def sklearn_topk_search(query_embs, snomed_embs, k=5, chunk_size=20000):\n",
        "    N = len(query_embs)\n",
        "    M = len(snomed_embs)\n",
        "    all_scores = np.full((N, k), -9999, dtype=\"float32\")\n",
        "    all_idx    = np.full((N, k), -1,    dtype=\"int32\")\n",
        "\n",
        "    for start in range(0, M, chunk_size):\n",
        "        end = min(start+chunk_size, M)\n",
        "        snomed_chunk = snomed_embs[start:end]\n",
        "        sims = cosine_similarity(query_embs, snomed_chunk)\n",
        "\n",
        "        local_topk = np.argpartition(-sims, k-1, axis=1)[:, :k]\n",
        "        local_scores = np.take_along_axis(sims, local_topk, axis=1)\n",
        "        global_idx = local_topk + start\n",
        "\n",
        "        comb_scores = np.concatenate([all_scores, local_scores], axis=1)\n",
        "        comb_idx    = np.concatenate([all_idx, global_idx], axis=1)\n",
        "\n",
        "        new_topk = np.argpartition(-comb_scores, k-1, axis=1)[:, :k]\n",
        "        all_scores = np.take_along_axis(comb_scores, new_topk, axis=1)\n",
        "        all_idx    = np.take_along_axis(comb_idx,    new_topk, axis=1)\n",
        "\n",
        "    return all_idx, all_scores\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 9. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + Re-ranking\n",
        "# (ë©”ëª¨ë¦¬ ë¬¸ì œ ë°©ì§€ ìœ„í•´ \"semantic-only ëª¨ë“œ\"ë¥¼ ê¸°ë³¸ìœ¼ë¡œ)\n",
        "# ================================\n",
        "USE_HYBRID = False      # Colabì—ì„œ ë©”ëª¨ë¦¬ ì•ˆì „ ìœ„í•´ ê¸°ë³¸ False\n",
        "HYBRID_ALPHA = 0.7\n",
        "USE_RERANK = True\n",
        "TOPK = 10\n",
        "\n",
        "def hybrid_search(query_texts, snomed_texts, snomed_embs, sapbert_model,\n",
        "                  alpha=0.7, k=10, use_faiss=True):\n",
        "    \"\"\"\n",
        "    (ì£¼ì˜) ì „ì²´ ì½”í¼ìŠ¤ì— ëŒ€í•´ full matrixë¥¼ ë§Œë“¤ê¸° ë•Œë¬¸ì—,\n",
        "    SNOMED ê·œëª¨ê°€ í¬ë©´ Colabì—ì„œ ë©”ëª¨ë¦¬ í„°ì§ˆ ìˆ˜ ìˆìŒ.\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘ (alpha={alpha}, k={k})\")\n",
        "\n",
        "    print(\"  ğŸ“Š TF-IDF ê³„ì‚° ì¤‘...\")\n",
        "    tfidf = TfidfVectorizer(max_features=5000, min_df=2)\n",
        "    snomed_tfidf = tfidf.fit_transform(snomed_texts)\n",
        "    query_tfidf = tfidf.transform(query_texts)\n",
        "\n",
        "    # full lexical similarity (ë©”ëª¨ë¦¬ ì£¼ì˜)\n",
        "    lexical_scores = cosine_similarity(query_tfidf, snomed_tfidf)\n",
        "\n",
        "    print(\"  ğŸ§  SapBERT ì„ë² ë”© ì¤‘...\")\n",
        "    query_embs = sapbert_model.encode(\n",
        "        query_texts,\n",
        "        batch_size=16,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    if use_faiss and FAISS_AVAILABLE:\n",
        "        print(\"  âš¡ FAISS ê²€ìƒ‰...\")\n",
        "        faiss_index = build_faiss_index(snomed_embs, use_gpu=(device == \"cuda\"))\n",
        "        semantic_scores = cosine_similarity(query_embs, snomed_embs)\n",
        "    else:\n",
        "        print(\"  ğŸ’» sklearn ê²€ìƒ‰...\")\n",
        "        semantic_scores = cosine_similarity(query_embs, snomed_embs)\n",
        "\n",
        "    print(\"  ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°...\")\n",
        "    hybrid_scores = alpha * semantic_scores + (1 - alpha) * lexical_scores\n",
        "\n",
        "    topk_idx = np.argsort(-hybrid_scores, axis=1)[:, :k]\n",
        "    topk_scores = np.take_along_axis(hybrid_scores, topk_idx, axis=1)\n",
        "\n",
        "    print(f\"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ\")\n",
        "    return topk_idx, topk_scores\n",
        "\n",
        "def rerank_with_heuristics(topk_idx, topk_scores, df_test, snomed_index):\n",
        "    print(\"ğŸ¯ Re-ranking ì‹œì‘...\")\n",
        "    reranked_idx = []\n",
        "    reranked_scores = []\n",
        "\n",
        "    for i in range(len(df_test)):\n",
        "        cand_indices = topk_idx[i]\n",
        "        cand_scores = topk_scores[i].copy()\n",
        "        query_len = len(df_test.iloc[i][\"kcd_normalized\"].split())\n",
        "\n",
        "        for j, idx in enumerate(cand_indices):\n",
        "            info = snomed_index.iloc[idx]\n",
        "\n",
        "            if info[\"term_type\"] == \"FSN\":\n",
        "                cand_scores[j] += 0.05\n",
        "\n",
        "            cand_len = len(str(info[\"normalized_term\"]).split())\n",
        "            if query_len > 0 and cand_len > 0:\n",
        "                len_sim = 1 - abs(query_len - cand_len) / max(query_len, cand_len)\n",
        "                cand_scores[j] += 0.03 * len_sim\n",
        "\n",
        "        sorted_order = np.argsort(-cand_scores)\n",
        "        reranked_idx.append(cand_indices[sorted_order])\n",
        "        reranked_scores.append(cand_scores[sorted_order])\n",
        "\n",
        "    print(\"âœ… Re-ranking ì™„ë£Œ\")\n",
        "    return np.array(reranked_idx), np.array(reranked_scores)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 10. ê²€ìƒ‰ ì‹¤í–‰ (ê¸°ë³¸: SapBERT-only)\n",
        "# ================================\n",
        "if USE_HYBRID:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“œ\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    topk_idx, topk_scores = hybrid_search(\n",
        "        df_test[\"kcd_normalized\"].tolist(),\n",
        "        snomed_index[\"normalized_term\"].tolist(),\n",
        "        snomed_embs,\n",
        "        sapbert,\n",
        "        alpha=HYBRID_ALPHA,\n",
        "        k=TOPK,\n",
        "        use_faiss=FAISS_AVAILABLE\n",
        "    )\n",
        "else:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ğŸ§  ìˆœìˆ˜ SapBERT ê²€ìƒ‰ ëª¨ë“œ\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    query_texts = df_test[\"kcd_no_plural\"].tolist()\n",
        "    query_embs = sapbert.encode(\n",
        "        query_texts,\n",
        "        batch_size=16,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True,\n",
        "        normalize_embeddings=True\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    if FAISS_AVAILABLE:\n",
        "        print(\"âš¡ FAISS ê²€ìƒ‰ ì¤‘...\")\n",
        "        faiss_index = build_faiss_index(snomed_embs, use_gpu=(device == \"cuda\"))\n",
        "        topk_idx, topk_scores = faiss_topk_search(faiss_index, query_embs, k=TOPK)\n",
        "    else:\n",
        "        print(\"ğŸ’» sklearn ê²€ìƒ‰ ì¤‘...\")\n",
        "        topk_idx, topk_scores = sklearn_topk_search(query_embs, snomed_embs, k=TOPK)\n",
        "\n",
        "if USE_RERANK:\n",
        "    topk_idx, topk_scores = rerank_with_heuristics(\n",
        "        topk_idx, topk_scores, df_test, snomed_index\n",
        "    )\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 11. ê²°ê³¼ ìƒì„±\n",
        "# ================================\n",
        "conceptIds = snomed_index[\"conceptId\"].tolist()\n",
        "terms      = snomed_index[\"term\"].tolist()\n",
        "\n",
        "df_pred = df_test.copy()\n",
        "df_pred[\"sapbert_best_idx\"] = [idxs[0] for idxs in topk_idx]\n",
        "df_pred[\"pred_conceptId\"]   = df_pred[\"sapbert_best_idx\"].apply(lambda x: conceptIds[x])\n",
        "df_pred[\"pred_term\"]        = df_pred[\"sapbert_best_idx\"].apply(lambda x: terms[x])\n",
        "df_pred[\"pred_score\"]       = [scores[0] for scores in topk_scores]\n",
        "df_pred[\"mapping_step\"]     = \"hybrid_rerank\" if USE_HYBRID else \"sapbert_only\"\n",
        "\n",
        "topk_pred_ids = []\n",
        "topk_pred_terms = []\n",
        "topk_pred_scores_list = []\n",
        "\n",
        "for i in range(len(df_test)):\n",
        "    idx_list = topk_idx[i]\n",
        "    cand_ids = [str(conceptIds[idx]) for idx in idx_list]\n",
        "    cand_terms = [str(terms[idx]) for idx in idx_list]\n",
        "    cand_scores = [float(s) for s in topk_scores[i]]\n",
        "\n",
        "    topk_pred_ids.append(cand_ids)\n",
        "    topk_pred_terms.append(cand_terms)\n",
        "    topk_pred_scores_list.append(cand_scores)\n",
        "\n",
        "df_pred[f\"top{TOPK}_conceptIds\"] = topk_pred_ids\n",
        "df_pred[f\"top{TOPK}_terms\"]      = topk_pred_terms\n",
        "df_pred[f\"top{TOPK}_scores\"]     = topk_pred_scores_list\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 12. ì •ë‹µ ë¡œë“œ + Accuracy/Top-K/MRR/NDCG/MAP/AvgRank\n",
        "# ================================\n",
        "target_file = os.path.join(TEST_DIR, \"test_target_with_source_code.xlsx\")\n",
        "df_target = pd.read_excel(target_file, dtype=str).fillna(\"\")\n",
        "\n",
        "df_target = df_target.rename(columns={\n",
        "    \"concept_code\": \"true_conceptId\",\n",
        "    \"concept_name\": \"true_term\"\n",
        "})\n",
        "\n",
        "df_eval = df_pred.merge(df_target, left_index=True, right_index=True)\n",
        "\n",
        "df_eval[\"correct\"] = (\n",
        "    df_eval[\"pred_conceptId\"].astype(str) == df_eval[\"true_conceptId\"].astype(str)\n",
        ")\n",
        "top1_accuracy = df_eval[\"correct\"].mean() * 100\n",
        "\n",
        "topk_correct = []\n",
        "for i, row in df_eval.iterrows():\n",
        "    true_id = str(row[\"true_conceptId\"])\n",
        "    cand_ids = row[f\"top{TOPK}_conceptIds\"]\n",
        "    topk_correct.append(true_id in cand_ids)\n",
        "\n",
        "df_eval[\"topk_correct\"] = topk_correct\n",
        "topk_accuracy = df_eval[\"topk_correct\"].mean() * 100\n",
        "\n",
        "ranks_rr = []\n",
        "for i in range(len(df_test)):\n",
        "    true_id = str(df_target.loc[i, \"true_conceptId\"])\n",
        "    cand_ids = df_pred.loc[i, f\"top{TOPK}_conceptIds\"]\n",
        "    rr = 0.0\n",
        "    for rank, cid in enumerate(cand_ids, start=1):\n",
        "        if cid == true_id:\n",
        "            rr = 1.0 / rank\n",
        "            break\n",
        "    ranks_rr.append(rr)\n",
        "MRR = sum(ranks_rr) / len(ranks_rr)\n",
        "\n",
        "def dcg_at_k(relevances, k):\n",
        "    relevances = np.asarray(relevances)[:k]\n",
        "    if relevances.size == 0:\n",
        "        return 0.0\n",
        "    discounts = 1.0 / np.log2(np.arange(2, relevances.size + 2))\n",
        "    return float(np.sum(relevances * discounts))\n",
        "\n",
        "def ndcg_at_k_binary(relevances, k):\n",
        "    dcg = dcg_at_k(relevances, k)\n",
        "    ideal_rels = [1] + [0] * (k - 1)\n",
        "    idcg = dcg_at_k(ideal_rels, k)\n",
        "    if idcg == 0:\n",
        "        return 0.0\n",
        "    return dcg / idcg\n",
        "\n",
        "def average_precision_at_k_binary(relevances, k):\n",
        "    relevances = np.asarray(relevances)[:k]\n",
        "    if np.sum(relevances) == 0:\n",
        "        return 0.0\n",
        "    precisions = []\n",
        "    num_hits = 0\n",
        "    for i, rel in enumerate(relevances, start=1):\n",
        "        if rel > 0:\n",
        "            num_hits += 1\n",
        "            precisions.append(num_hits / i)\n",
        "    if len(precisions) == 0:\n",
        "        return 0.0\n",
        "    return float(np.mean(precisions))\n",
        "\n",
        "K_MAX = TOPK\n",
        "acc_list = []\n",
        "for k in range(1, TOPK + 1):\n",
        "    correct_at_k = 0\n",
        "    for i in range(len(df_test)):\n",
        "        true_id = str(df_target.loc[i, \"true_conceptId\"])\n",
        "        cand_ids = df_pred.loc[i, f\"top{TOPK}_conceptIds\"][:k]\n",
        "        if true_id in cand_ids:\n",
        "            correct_at_k += 1\n",
        "    acc_list.append(correct_at_k / len(df_test))\n",
        "\n",
        "top1_acc = acc_list[0]\n",
        "top5_acc = acc_list[min(4, K_MAX - 1)]\n",
        "top10_acc = acc_list[min(9, K_MAX - 1)]\n",
        "\n",
        "ndcg5_list = []\n",
        "ndcg10_list = []\n",
        "ap10_list = []\n",
        "avg_rank_list = []\n",
        "\n",
        "for i in range(len(df_test)):\n",
        "    true_id = str(df_target.loc[i, \"true_conceptId\"])\n",
        "    cand_ids_full = df_pred.loc[i, f\"top{TOPK}_conceptIds\"]\n",
        "    relevances = [1 if cid == true_id else 0 for cid in cand_ids_full]\n",
        "\n",
        "    ndcg5  = ndcg_at_k_binary(relevances, k=min(5, K_MAX))\n",
        "    ndcg10 = ndcg_at_k_binary(relevances, k=min(10, K_MAX))\n",
        "    ndcg5_list.append(ndcg5)\n",
        "    ndcg10_list.append(ndcg10)\n",
        "\n",
        "    ap10 = average_precision_at_k_binary(relevances, k=min(10, K_MAX))\n",
        "    ap10_list.append(ap10)\n",
        "\n",
        "    rank = 0\n",
        "    for r, cid in enumerate(cand_ids_full, start=1):\n",
        "        if cid == true_id:\n",
        "            rank = r\n",
        "            break\n",
        "    if rank == 0:\n",
        "        rank = K_MAX + 1\n",
        "    avg_rank_list.append(rank)\n",
        "\n",
        "ndcg5_mean = float(np.mean(ndcg5_list))\n",
        "ndcg10_mean = float(np.mean(ndcg10_list))\n",
        "map10_mean = float(np.mean(ap10_list))\n",
        "avg_rank = float(np.mean(avg_rank_list))\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€ ê²°ê³¼\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"ğŸ¯ Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
        "print(f\"ğŸ¯ Top-{TOPK} Accuracy: {topk_accuracy:.2f}%\")\n",
        "print(f\"ğŸ¯ MRR (Mean Reciprocal Rank): {MRR:.4f}\")\n",
        "print(f\"ğŸ¯ Accuracy@5: {top5_acc*100:.2f}%\")\n",
        "print(f\"ğŸ¯ Accuracy@10: {top10_acc*100:.2f}%\")\n",
        "print(f\"ğŸ¯ NDCG@5: {ndcg5_mean:.4f}\")\n",
        "print(f\"ğŸ¯ NDCG@10: {ndcg10_mean:.4f}\")\n",
        "print(f\"ğŸ¯ MAP@10: {map10_mean:.4f}\")\n",
        "print(f\"ğŸ¯ Average Rank: {avg_rank:.2f}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 13. ì˜¤ë¥˜ ë¶„ì„\n",
        "# ================================\n",
        "def analyze_errors(df_eval, snomed_index, top_n=10):\n",
        "    errors = df_eval[~df_eval[\"correct\"]].copy()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"âŒ ì˜¤ë¥˜ ë¶„ì„\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"ì´ ì˜¤ë¥˜ ê°œìˆ˜: {len(errors)} / {len(df_eval)}\")\n",
        "    print(f\"ì •í™•ë„: {df_eval['correct'].mean() * 100:.2f}%\\n\")\n",
        "\n",
        "    if len(errors) == 0:\n",
        "        print(\"ğŸ‰ ì˜¤ë¥˜ ì—†ìŒ!\")\n",
        "        return errors\n",
        "\n",
        "    print(f\"ğŸ” High-Confidence ì˜¤ë¥˜ (Top {min(top_n, len(errors))}):\")\n",
        "    confident_errors = errors.nlargest(min(top_n, len(errors)), \"pred_score\")\n",
        "    for idx, row in confident_errors.iterrows():\n",
        "        print(f\"\\n  [{row['kcd_code']}] {row['kcd_en']}\")\n",
        "        print(f\"    ì˜ˆì¸¡: {row['pred_conceptId']} - {row['pred_term']}\")\n",
        "        print(f\"    ì ìˆ˜: {row['pred_score']:.3f}\")\n",
        "        print(f\"    ì •ë‹µ: {row['true_conceptId']} - {row['true_term']}\")\n",
        "\n",
        "    print(f\"\\n\\nğŸ¤” Low-Confidence ì˜¤ë¥˜ (Bottom {min(top_n, len(errors))}):\")\n",
        "    uncertain = errors.nsmallest(min(top_n, len(errors)), \"pred_score\")\n",
        "    for idx, row in uncertain.head(5).iterrows():\n",
        "        print(f\"\\n  [{row['kcd_code']}] {row['kcd_en']}\")\n",
        "        print(f\"    ì˜ˆì¸¡ ì ìˆ˜: {row['pred_score']:.3f}\")\n",
        "        print(f\"    ì •ë‹µ: {row['true_term']}\")\n",
        "\n",
        "    print(f\"\\n\\nğŸ“Š ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ì ìˆ˜ í†µê³„:\")\n",
        "    print(f\"  í‰ê· : {errors['pred_score'].mean():.3f}\")\n",
        "    print(f\"  ì¤‘ì•™ê°’: {errors['pred_score'].median():.3f}\")\n",
        "    print(f\"  ìµœì†Œ: {errors['pred_score'].min():.3f}\")\n",
        "    print(f\"  ìµœëŒ€: {errors['pred_score'].max():.3f}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    return errors\n",
        "\n",
        "errors_df = analyze_errors(df_eval, snomed_index, top_n=10)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 14. ì‹œê°í™”\n",
        "# ================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rank_list = []\n",
        "for i in range(len(df_test)):\n",
        "    true_id = str(df_target.loc[i, \"true_conceptId\"])\n",
        "    cand_ids = df_pred.loc[i, f\"top{TOPK}_conceptIds\"]\n",
        "    rank = 0\n",
        "    for r, cid in enumerate(cand_ids, start=1):\n",
        "        if cid == true_id:\n",
        "            rank = r\n",
        "            break\n",
        "    rank_list.append(rank)\n",
        "\n",
        "rank_counts = {}\n",
        "for r in rank_list:\n",
        "    rank_counts[r] = rank_counts.get(r, 0) + 1\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "ax0 = axes[0, 0]\n",
        "ax0.plot(range(1, TOPK + 1), [a * 100 for a in acc_list], marker='o', linewidth=2)\n",
        "ax0.set_xlabel('K', fontsize=12)\n",
        "ax0.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax0.set_title('Accuracy@K Curve', fontsize=14, fontweight='bold')\n",
        "ax0.grid(True, alpha=0.3)\n",
        "ax0.set_xticks(range(1, TOPK + 1))\n",
        "\n",
        "ax1 = axes[0, 1]\n",
        "ranks = sorted([r for r in rank_counts.keys() if r != 0])\n",
        "counts = [rank_counts[r] for r in ranks]\n",
        "ax1.bar(ranks, counts, color='steelblue', alpha=0.7)\n",
        "ax1.set_xlabel('Rank of Correct Answer', fontsize=12)\n",
        "ax1.set_ylabel('Number of Samples', fontsize=12)\n",
        "ax1.set_title('Distribution of Correct Answer Rank', fontsize=14, fontweight='bold')\n",
        "ax1.set_xticks(ranks)\n",
        "\n",
        "ax2 = axes[1, 0]\n",
        "top1_scores = [float(scores[0]) for scores in topk_scores]\n",
        "ax2.hist(top1_scores, bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
        "ax2.set_xlabel('Top-1 Similarity Score', fontsize=12)\n",
        "ax2.set_ylabel('Number of Samples', fontsize=12)\n",
        "ax2.set_title('Top-1 Score Distribution', fontsize=14, fontweight='bold')\n",
        "ax2.axvline(np.mean(top1_scores), color='red', linestyle='--',\n",
        "            linewidth=2, label=f'Mean: {np.mean(top1_scores):.3f}')\n",
        "ax2.legend()\n",
        "\n",
        "ax3 = axes[1, 1]\n",
        "metric_names = ['Acc@1', 'Acc@5', 'Acc@10', 'MRR', 'NDCG@5', 'NDCG@10', 'MAP@10']\n",
        "metric_vals = [\n",
        "    top1_acc * 100,\n",
        "    top5_acc * 100,\n",
        "    top10_acc * 100,\n",
        "    MRR * 100,\n",
        "    ndcg5_mean * 100,\n",
        "    ndcg10_mean * 100,\n",
        "    map10_mean * 100\n",
        "]\n",
        "ax3.bar(metric_names, metric_vals, color='seagreen', alpha=0.8)\n",
        "ax3.set_ylabel('Score (%)', fontsize=12)\n",
        "ax3.set_title('Ranking Metrics Summary', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylim(0, 100)\n",
        "for x, v in zip(metric_names, metric_vals):\n",
        "    ax3.text(x, v + 1, f\"{v:.1f}\", ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "viz_path = os.path.join(TEST_DIR, \"performance_visualization_extended.png\")\n",
        "plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"ğŸ“ í™•ì¥ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {viz_path}\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 15. ê²°ê³¼ ì €ì¥\n",
        "# ================================\n",
        "output_main = os.path.join(TEST_DIR, \"improved_sapbert_result.xlsx\")\n",
        "output_errors = os.path.join(TEST_DIR, \"error_analysis.xlsx\")\n",
        "output_metrics = os.path.join(TEST_DIR, \"performance_metrics.csv\")\n",
        "output_extra_metrics = os.path.join(TEST_DIR, \"performance_metrics_extended.csv\")\n",
        "\n",
        "df_eval.to_excel(output_main, index=False)\n",
        "print(f\"ğŸ“ ì „ì²´ ê²°ê³¼ ì €ì¥: {output_main}\")\n",
        "\n",
        "if len(errors_df) > 0:\n",
        "    errors_df.to_excel(output_errors, index=False)\n",
        "    print(f\"ğŸ“ ì˜¤ë¥˜ ë¶„ì„ ì €ì¥: {output_errors}\")\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'K': list(range(1, TOPK + 1)),\n",
        "    'Accuracy': [a * 100 for a in acc_list]\n",
        "})\n",
        "metrics_df.to_csv(output_metrics, index=False)\n",
        "print(f\"ğŸ“ ì„±ëŠ¥ ì§€í‘œ (Accuracy@K) ì €ì¥: {output_metrics}\")\n",
        "\n",
        "extra_metrics = {\n",
        "    'Top1_Accuracy': top1_acc * 100,\n",
        "    'Top5_Accuracy': top5_acc * 100,\n",
        "    'Top10_Accuracy': top10_acc * 100,\n",
        "    'MRR': MRR,\n",
        "    'NDCG@5': ndcg5_mean,\n",
        "    'NDCG@10': ndcg10_mean,\n",
        "    'MAP@10': map10_mean,\n",
        "    'AvgRank': avg_rank\n",
        "}\n",
        "extra_metrics_df = pd.DataFrame([extra_metrics])\n",
        "extra_metrics_df.to_csv(output_extra_metrics, index=False)\n",
        "print(f\"ğŸ“ í™•ì¥ ì„±ëŠ¥ ì§€í‘œ ì €ì¥: {output_extra_metrics}\")\n",
        "\n",
        "summary_path = os.path.join(TEST_DIR, \"performance_summary.txt\")\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\"*60 + \"\\n\")\n",
        "    f.write(\"ê°œì„ ëœ SapBERT ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "    f.write(f\"ê²€ìƒ‰ ëª¨ë“œ: {'í•˜ì´ë¸Œë¦¬ë“œ (Lexical + Semantic)' if USE_HYBRID else 'ìˆœìˆ˜ SapBERT'}\\n\")\n",
        "    if USE_HYBRID:\n",
        "        f.write(f\"í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ (alpha): {HYBRID_ALPHA}\\n\")\n",
        "    f.write(f\"Re-ranking ì ìš©: {'ì˜ˆ' if USE_RERANK else 'ì•„ë‹ˆì˜¤'}\\n\")\n",
        "    f.write(f\"FAISS ì‚¬ìš©: {'ì˜ˆ' if FAISS_AVAILABLE else 'ì•„ë‹ˆì˜¤ (sklearn)'}\\n\\n\")\n",
        "    f.write(\"-\"*60 + \"\\n\")\n",
        "    f.write(\"í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ\\n\")\n",
        "    f.write(\"-\"*60 + \"\\n\")\n",
        "    f.write(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\\n\")\n",
        "    f.write(f\"Top-{TOPK} Accuracy: {topk_accuracy:.2f}%\\n\")\n",
        "    f.write(f\"MRR: {MRR:.4f}\\n\")\n",
        "    f.write(f\"Accuracy@5: {top5_acc*100:.2f}%\\n\")\n",
        "    f.write(f\"Accuracy@10: {top10_acc*100:.2f}%\\n\")\n",
        "    f.write(f\"NDCG@5: {ndcg5_mean:.4f}\\n\")\n",
        "    f.write(f\"NDCG@10: {ndcg10_mean:.4f}\\n\")\n",
        "    f.write(f\"MAP@10: {map10_mean:.4f}\\n\")\n",
        "    f.write(f\"Average Rank: {avg_rank:.2f}\\n\\n\")\n",
        "    f.write(\"-\"*60 + \"\\n\")\n",
        "    f.write(\"Accuracy@K ìƒì„¸\\n\")\n",
        "    f.write(\"-\"*60 + \"\\n\")\n",
        "    for k, acc in enumerate(acc_list, 1):\n",
        "        f.write(f\"Accuracy@{k}: {acc * 100:.2f}%\\n\")\n",
        "    f.write(\"\\n\" + \"-\"*60 + \"\\n\")\n",
        "    f.write(\"Top-1 Score í†µê³„\\n\")\n",
        "    f.write(\"-\"*60 + \"\\n\")\n",
        "    f.write(f\"í‰ê· : {np.mean(top1_scores):.4f}\\n\")\n",
        "    f.write(f\"ì¤‘ì•™ê°’: {np.median(top1_scores):.4f}\\n\")\n",
        "    f.write(f\"í‘œì¤€í¸ì°¨: {np.std(top1_scores):.4f}\\n\")\n",
        "    f.write(f\"ìµœì†Œ: {np.min(top1_scores):.4f}\\n\")\n",
        "    f.write(f\"ìµœëŒ€: {np.max(top1_scores):.4f}\\n\")\n",
        "\n",
        "print(f\"ğŸ“ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {summary_path}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(\"ì €ì¥ëœ íŒŒì¼:\")\n",
        "print(f\"  - {output_main}\")\n",
        "print(f\"  - {output_errors}\")\n",
        "print(f\"  - {output_metrics}\")\n",
        "print(f\"  - {output_extra_metrics}\")\n",
        "print(f\"  - {summary_path}\")\n",
        "print(f\"  - {viz_path}\")\n"
      ]
    }
  ]
}